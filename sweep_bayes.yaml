
name: test_sweep

command:
  - python
  - ${program}
  - "-c"
  - sweep_base_config.yaml

metric:
  goal: minimize
  name: test_epoch_PER

program: pretrain.py

method: bayes

early_terminate:
  type: hyperband
  min_iter: 20
  eta: 2

parameters:
  trainer.num_epochs:
    values: [50,100]
  trainer.train_batch_size:
    values: [16, 64, 128]
  optimizer.lr:
    max: -2.99573227355
    min: -11.512925465
    distribution: log_uniform
  optimizer.wd:
    max: -2.30258509299
    min: -15.7232658369
    distribution: log_uniform
  optimizer.warmup_epochs:
    max: 25
    min: 0
  neural_encoder.context.forward:
    values: [-1, 0, 10, 50]
  neural_encoder.context.backward:
    values: [-1, 0, 10, 50]
  neural_encoder.norm_and_noise.normalize:
    values: [False, True]
  neural_encoder.norm_and_noise.white_noise_SD:
    max: 2.0
    min: 0.0
  neural_encoder.norm_and_noise.constant_offset_SD:
    max: 2.0
    min: 0.0
  neural_encoder.embedder.mult:
    values: [1, 2, 3]
  neural_encoder.embedder.adapt:
    values: [true, false]
  neural_encoder.embedder.stack.size:
    max: 64
    min: 1
  neural_encoder.embedder.stack.stride:
    max: 12
    min: 1
  neural_encoder.embedder.dropout:
    max: 0.6
    min: 0.0
  neural_encoder.transformer.n_layers:
    max: 15
    min: 2
  neural_encoder.transformer.hidden_size:
    values: [512, 1024, 1536, 2048]
  neural_encoder.transformer.inter_size:
    values: [512, 1024, 1536, 2048, 4096]
  neural_encoder.transformer.n_heads:
    values: [2, 8, 16, 32]
  neural_encoder.transformer.dropout:
    max: 0.6
    min: 0.0
  neural_encoder.factors.dropout:
    max: 0.6
    min: 0.0