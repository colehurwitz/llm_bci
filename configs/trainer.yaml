seed: 1

savestring: poisson-pretrain  
wandb_project: itransformer
log_to_wandb: false

verbosity: 0

dirs:
  checkpoint_dir: /home/gridsan/dbeneto/TFG/BCI/pt_checkpoints
  log_dir: /home/gridsan/dbeneto/TFG/BCI/pt_logs

## These two fields are not used in Trainer
model: include:configs/patchtst.yaml
dataset:
  hf_dataset_name: null
  json_dataset_name: null
  dir: /home/gridsan/dbeneto/MAML-Soljacic_shared/BCI/data/competitionData
  zscore: True
  vocab_file: vocab.json
## 

method:
  model_kwargs:
    method_name: ctc     # ssl
    vocab_size: 41
    blank_id: 0
    zero_infinity: false
    
    use_lograte: true
    loss: mse   # poisson_nll

  dataset_kwargs:
    dataset_name: ctc
    target_name: phonemes

  dataloader_kwargs:
    pad_dict:
      spikes:
        dim: 0
        value: 0
      targets:
        dim: 0
        value: 0

trainer:
  train_name: train
  test_name: test
  train_len: null # null to use all
  test_len: null
  shuffle_test_dataloader: false
  
optimizer:
  gradient_accumulation_steps: 4
  lr: 1.e-4
  wd: 0.01
  eps: 1.e-8
  warmup_pct: 0.0 # cosine/linear
  gamma: 0.95     # step
  div_factor: 10  # cosine
  scheduler: step # step/cosine/linear
