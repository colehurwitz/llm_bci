savestring: "NDT2"

path_to_data: /home/llm4bci/competitionData/processed.data
path_to_model: /home/llm4bci/LLM
checkpoint_dir: /home/llm4bci/checkpoints/
ft_dir: /home/llm4bci/ft_models/
log_dir: /home/llm4bci/logs/


bci: 
  stacking: 16
  neural_config: 
    context_forward: 15
    


trainer:
  num_epochs: 25
  save_epochs: [1,2,5,10,15,20]
  save_every: null
  train_len: 12
  test_len: 12
  test_batch_size: 3
  train_batch_size: 3

optimizer:
  lr: 3.e-4
  wd: 0.01
  warmup_ratio: 0.05

lora:
  r: 8
  alpha: 32
  dropout: 0.1
