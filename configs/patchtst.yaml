model_class: PatchTST

encoder:
  from_pt: null
  num_input_channels: 128
  context_length: 45
  patch_length: 10
  patch_stride: 10
  num_hidden_layers: 4
  d_model: 256
  num_attention_heads: 8
  share_embedding: True
  channel_attention: False
  ffn_dim: 1024
  norm_type: "batchnorm" 
  norm_eps: 1.e-5
  attention_dropout: 0.4
  # dropout: 0.2    # for pretrain head
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.4
  bias: True
  activation_function: gelu
  pre_norm: True
  positional_encoding_type: sincos
  # use_cls_token: False
  init_std: 0.02
  scaling: null # std/mean/null
  do_mask_input: True
  mask_type: random
  random_mask_ratio: 0.1
  channel_consistent_masking: false
  # unmasked_channel_indices
  mask_value: 0


decoder:
  from_pt: null
  share_projection: true
  pooling_type: mean
  head_dropout: 0.0
  mlp_decoder: false
  mlp_activation: gelu
  # distribution_output 
  # loss: None,
  # num_forecast_mask_patches:
  # prediction_length: 
  # num_targets
  # output_range
  # num_parallel_samples
  


  







