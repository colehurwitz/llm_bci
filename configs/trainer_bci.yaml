seed: 1

savestring: test
wandb_project: test
log_to_wandb: false

verbosity: 0

# Logging directories
dirs:
  checkpoint_dir: /home/gridsan/dbeneto/TFG/BCI/bci_checkpoints  # save model state dicts (todo optimizer states)
  log_dir: /home/gridsan/dbeneto/TFG/BCI/bci_logs  # save tensorboard logs


# Training configuration
training:
  num_epochs: 10
  train_batch_size: 1
  test_batch_size: 1      
  shuffle_test_dataloader: false    # Shuffle test dataloader between epochs
  drop_last_test_dataloader: true   # Drop last batch to ensure same batch size
  drop_last_train_dataloader: false   # Drop last batch to ensure same batch size
  save_every: 500  # Save checkpoint
  eval_every: 500  # Eval model

model: include:configs/bci.yaml

data:
  dataset_class: decoding

  hf_dataset_name: null
  json_dataset_name: null

  train_name: train   # name of the train split in the raw datasete
  test_name: test     # name of the test split in the raw datasete
  train_len: null     # used length of the train dataset. null to use all
  test_len: null      # used length of the test dataset. null to use all

  data_load: speechbci
  data_dir: /home/gridsan/dbeneto/MAML-Soljacic_shared/BCI/data/competitionData
  zscore_day: True
  zscore_block: False
  date_idxs: null
  tokenizer_path: /home/gridsan/dbeneto/MAML-Soljacic_shared/llama2/tokenizer
  prompt: "neural activity:#-> sentence:"

method:

  model_kwargs:
    method_name: endtoend

    load_ndt1_from_pt: "pt_checkpoints/ndt1-ctc-seed_4-opt_64_1.e-3_5.e-5-arch_5_1024_1024-d_0.4_0.2-noise_true-smooth_null-context_false_false_false-dsdecoding_false_true/STEP8800"
    llm_path: /home/gridsan/dbeneto/MAML-Soljacic_shared/llama2/7b
    lora:
      r: 8
      alpha: 32
      dropout: 0.2
      target_modules: ["q_proj","v_proj","k_proj","o_proj","gate_proj","up_proj","down_proj"]
      modules_to_save: []
    freeze_llm: false

    vocab_size: 41
    blank_id: 0
    zero_infinity: true

  dataset_kwargs:
    targets_name: labels

  dataloader_kwargs:
    pad_dict:
      spikes:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null
      spikes_mask:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null
      spikes_timestamp:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null
      targets:
        dim: 0
        side: right
        value: -100
        truncate: null
        min_length: null
      targets_mask:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null
      input_ids:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null
      attention_mask:
        dim: 0
        side: right
        value: 0
        truncate: null
        min_length: null


  metric_kwargs:
    n_print: 1

optimizer:
  gradient_accumulation_steps: 1
  lr: 1.e-4
  wd: 5.e-5
  eps: 1.e-8
  warmup_pct: 0.0 # cosine/linear
  gamma: 0.95     # step
  div_factor: 25  # cosine
  scheduler: step # step/cosine/linear
