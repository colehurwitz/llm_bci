model_class: NDT1


encoder:

  from_pt: null

  # Mask spikes
  masker:
    neuron:
      active: false        
      mode: neuron        # masking mode
      ratio: 0.1          # ratio of data to predict
      zero_ratio: 1.0     # of the data to predict, ratio of zeroed out
      random_ratio: 1.0   # of the not zeroed, ratio of randomly replaced
      expand_prob: 0.0    # probability of expanding the mask in ``temporal`` mode
      max_timespan: 1     # max span of mask if expanded
      regions: null       # brain regions to mask in ``region`` mode
      channels: null      # neurons to mask in ``co-smooth`` mode

  # Context available for each timestep
  context:
    forward: -2         # -1 for masking self, -2 for full
    backward: -2        # -1 for masking self, -2 for full

  # Normalize and add noise
  norm_and_noise:
    active: true
    smooth_sd: 2                # gaussian smoohing  
    norm: null                  # which normalization layer to use (null/layernorm/zscore)
    eps: 1.e-7                  # avoid dividing by zero when normalizing padded spikes     
    white_noise_sd: 1.0        # 1.0 gaussian noise added to the inputs  1.0 originally
    constant_offset_sd: 0.2    # 0.2 gaussian noise added to the inputs but contsnat in the time dimension 0.2 originally
    

  # Embedding layer
  embedder:
    n_channels: 256       # number of neurons recorded 
    n_blocks: 24          # number of blocks of experiments
    n_days: 24            # number of days of experiments
    max_F: 1024           # max spikes sequence len in timesteps

    input_dim: 512        # embedding size of each tiemstep before stacking
    adapt: false          # adapt the embedding layer for each day
    day_token: false      # add day token embedding at the beginning
    block_token: false    # add block token embedding at the beginning
    pos: true             # embed position 
    act: softsign         # activation for the embedding layers
    bias: true            # use bias in the embedding layer
    dropout: 0.2          # dropout in embedding layer

    stack:
      active: true        # wether to stack consecutive timesteps
      size: 32            # number of consecutive timesteps to stack
      stride: 4           # stacking stride


  # Transformer
  transformer:
    n_layers: 5           # number of transformer layers
    hidden_size: 1024     # hidden space of the transformer
    use_rope: false       # use rotary postional encoding
    rope_theta: 10000.0   # rope angle of rotation

    n_heads: 8            # number of attentiomn heads
    attention_bias: true  # learn bias in the attention layers

    act: gelu             # activiation function in mlp layers
    inter_size: 1024      # intermediate dimension in the mlp layers
    mlp_bias: true        # learn bias in the mlp layers
    
    dropout: 0.4          # dropout in transformer layers
    fixup_init: true      # modify weight initialization

  # Projection to factor space
  factors:  
    active: false             # project from hidden_size to factors
    size:   1024              # factors size  
    act: relu                 # activation function after projecting to factors
    bias: true                # use bias in projection to factors
    dropout: 0.0              # dropout in projection to factors
    fixup_init: false         # modify weight initialization
    init_range: 0.1           # initialization range for factors projetion
    
decoder:
  from_pt: null

  






