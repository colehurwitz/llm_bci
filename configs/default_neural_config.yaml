# Mask features
masker:
  active: false         
  mode: timestep      # full to zero out randomly in the featue matrix. timestep to zero out randomly full timesteps. neuron to zero out full neurons
  ratio: 0.0          # ratio of data to predict
  zero_ratio: 1.0     # of the data to predict, ratio of zeroed out
  random_ratio: 1.0   # of the not zeroed, ratio of randomly replaced
  expand_prob: 0.0    # probability of expanding the mask 
  max_timespan: 1     # max span of mask if expanded

# Context available for each timestep
context:
  forward: -1
  backward: -1

# Normalize and add noise
norm_and_noise:
  active: false
  normalize: false            # normalize the data across the channels before embedding
  norm: "zscore"              # which normalization layer to use (layernorm/scalenorm/zscore)
  eps: 1.e-7                  # avoid dividing by zero when normalizing padded features     
  white_noise_SD: null        # gaussian noise added to the inputs  1.0 originally
  constant_offset_SD: null    # gaussian noise added to the inputs but contsnat in the time dimension 0.2 originally


# Embedding layer
embedder:
  n_channels: 256       # number of neurons recorded 
  n_blocks: 24          # number of blocks of experiments
  n_dates: 24           # number of days of experiments
  max_F: 1024           # max feature len in timesteps

  mode: embed           # linear/embed/identity
  mult: 1               # embedding multiplier. hiddden_sizd = n_channels * mult
  adapt: false          # adapt the embedding layer for each day
  pos: true             # embed position 
  act: identity         # activation for the embedding layers
  scale: 1              # scale the embedding multiplying by this number
  bias: true            # use bias in the embedding layer
  dropout: 0.5          # dropout in embedding layer
  
  fixup_init: false     # modify weight initialization
  init_range: 0.1       # initialization range for embeddings
  spike_log_init: false # special initialization 
  max_spikes: 10        # max number of spikes in a single time bin

  stack:
    active: false       # wether to stack consecutive timesteps
    size: 32            # numbe of consecutive timesteps to stack
    stride: 4           # stacking stride


# Transformer
transformer:
  n_layers: 4           # number of transformer layers
  hidden_size: 1024     # hidden space of the transformer
  use_scalenorm: false  # use scalenorm  instead of layernorm
  use_rope: false       # use rotary postional encoding
  rope_theta: 10000.0   # rope angle of rotation
  
  norm_bias: true       # learn bias in the normalizaton layers

  n_heads: 8            # number of attentiomn heads
  attention_bias: true  # learn bias in the attention layers

  act: gelu             # activiation function in mlp layers
  inter_size: 128       # intermediate dimension in the mlp layers
  mlp_bias: true        # learn bias in the mlp layers
  
  dropout: 0.4          # dropout in transformer layers
  fixup_init: true      # modify weight initialization

# Projection to factor space
factors:  
  project_to_factors: false  # project from hidden_size to factors
  size: 16                  # factors size  
  act: relu                 # activation function after projecting to factors
  bias: true                # use bias in projection to factors
  dropout: 0.4              # dropout in projection to factors
  fixup_init: false         # modify weight initialization
  init_range: 0.1           # initialization range for factors projetion
  







