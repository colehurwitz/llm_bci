{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2a31cc-3bca-45a8-a322-8df2da70d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/gridsan/dbeneto/TFG/BCI\")\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from g2p_en import G2p\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from models.phoneme_llm import PhonemeLLM\n",
    "from utils.config_utils import DictConfig, update_config\n",
    "from utils.data_utils import PhonemesFinetuneDataset, ft_pad_collate_fn, prepare_phonemes_data\n",
    "from utils.eval_utils import word_error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817693a0-a376-48e8-ba98-6fc827da1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"phonemes: %% sentence:\"\n",
    "checkpoint_dir = \"/n/home07/djimenezbeneto/lab/BCI/checkpoints\"\n",
    "checkpoint_dir = \"/home/gridsan/dbeneto/TFG/BCI/checkpoints\"\n",
    "\n",
    "savestring = \"new_ft/eval_64-accum_4-rank_1-lr_1.e-4-gauss_0.0-spikes_0.6_2_0.8-norm_identity_1\"\n",
    "STEP = 36249\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_dir, savestring)\n",
    "load_dir = os.path.join(checkpoint_dir,f\"STEP{STEP}\")\n",
    "config = DictConfig(torch.load(os.path.join(load_dir, \"config.pth\")))\n",
    "\n",
    "# config = update_config(config, \"iaifi_dirs.yaml\")\n",
    "# config = update_config(config, \"configs/sc_dirs.yaml\")\n",
    "llama_dir = config.dirs.llm_dir\n",
    "tokenizer_dir = config.dirs.tokenizer_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cfef25-4afb-4f34-99df-06e227f27755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cbd7bc214b453994b4f92e1ae25829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/dbeneto/.conda/envs/bci/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'configs/phoneme_coupler.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mdirs\u001b[38;5;241m.\u001b[39mllm_dir)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPhonemeLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m adapter_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(load_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(adapter_file):\n",
      "File \u001b[0;32m~/TFG/BCI/models/phoneme_llm.py:52\u001b[0m, in \u001b[0;36mPhonemeLLM.__init__\u001b[0;34m(self, llm, coupler_config_or_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     coupler_config \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(coupler_config_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoupler_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEFAULT_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoupler_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39minter_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     56\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39minter_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39mbias),\n\u001b[1;32m     57\u001b[0m         ACT2FN[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39mact],\n\u001b[1;32m     58\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39minter_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config\u001b[38;5;241m.\u001b[39mhidden_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoupler_config\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m     59\u001b[0m     )\n",
      "File \u001b[0;32m~/TFG/BCI/utils/config_utils.py:62\u001b[0m, in \u001b[0;36mupdate_config\u001b[0;34m(default_config, config)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_config\u001b[39m(default_config, config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(default_config, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m         default_config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdefault_config\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# If no config is provided, we iterate using the same config to make sure that the includes\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# are unpacked\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     config \u001b[38;5;241m=\u001b[39m default_config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m config\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'configs/phoneme_coupler.yaml'"
     ]
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(config.dirs.llm_dir)\n",
    "model = PhonemeLLM(llm, load_dir)\n",
    "adapter_file = os.path.join(load_dir, \"adapter_config.json\")\n",
    "if os.path.isfile(adapter_file):\n",
    "    model.load_adapter(load_dir, is_trainable=False)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8160942-7ee9-4fa2-9969-e86e64fc2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.dirs.tokenizer_dir, padding_side='left', add_bos_token=False, add_eos_token=False)\n",
    "pad_id = tokenizer.eos_token_id\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534a61f-5e2d-426e-9f02-5878d82c5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"trainer\"][\"test_len\"] = 20\n",
    "data = torch.load(os.path.join(config.dirs.data_dir, config.data_file))\n",
    "train_data = {k: v[:config.trainer.train_len] if config.trainer.train_len != -1 else v for k,v in data[\"train\"].items()}\n",
    "train_data = prepare_phonemes_data(train_data, tokenizer, g2p, config.prompt)\n",
    "test_data = {k: v[:config.trainer.test_len] if config.trainer.test_len != -1 else v for k,v in data[\"test\"].items()}\n",
    "test_data = prepare_phonemes_data(test_data, tokenizer, g2p, config.prompt)\n",
    "train_dataset = PhonemesFinetuneDataset(train_data)\n",
    "test_dataset = PhonemesFinetuneDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=partial(ft_pad_collate_fn,config.noise,config.mask,pad_id,\"test\"), batch_size=1, pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=partial(ft_pad_collate_fn,config.noise,config.mask,pad_id,\"test\"), batch_size=1, pin_memory=True,\n",
    ")\n",
    "\n",
    "train_iter = iter(train_dataloader)\n",
    "test_iter = iter(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f962d9c-a6fa-4c0e-af92-5b97e421a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    " model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09bfecb-baf7-41f6-b86b-471d1c34d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "beams = 1\n",
    "gen_config = {\n",
    "    \"max_new_tokens\": 20, \n",
    "    \"do_sample\": False, \"temperature\": 1.0,  #\"top_p\": 1.0, \n",
    "    \"num_beams\": beams, \n",
    "    \"num_beam_groups\": beams, \"diversity_penalty\": 1.2,\n",
    "    \"repetition_penalty\": 0.0, \"length_penalty\": 0.0, \"no_repeat_ngram_size\": None, \n",
    "    \"renormalize_logits\": True, \n",
    "    \"low_memory\": True,\n",
    "    \"num_return_sequences\": beams, \"output_scores\": True, \"return_dict_in_generate\": True,\n",
    "    \"pad_token_id\": pad_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52373ec-8f96-4a1d-8d09-0a7ff82e5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "all_pairs = []\n",
    "all_sentences = []\n",
    "all_errors = []\n",
    "all_words = []\n",
    "all_scores = []\n",
    "time_b = 0.\n",
    "time_c = 0.\n",
    "for i, (model_inputs, prompt_inputs, sentence, true_ph, pred_ph) in tqdm(enumerate(test_dataloader)):\n",
    "    a = perf_counter()\n",
    "    prompt_inputs = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else [sub_v.to(\"cuda\") for sub_v in v] for k,v in prompt_inputs.items()}\n",
    "    preds = model.predict(**prompt_inputs, **gen_config, synced_gpus=None)\n",
    "    b = perf_counter()\n",
    "    time_b += b-a \n",
    "    dec_preds = [tokenizer.decode(p.detach().cpu().squeeze(), skip_special_tokens=True) for i, p in enumerate(preds.sequences)]\n",
    "    print(dec_preds)\n",
    "    scores = preds.sequences_scores\n",
    "    scores = (scores-scores.mean())/scores.std()\n",
    "    pairs = sorted([(a.item(), b) for a,b in zip(scores, dec_preds)], key=lambda x: -x[0])\n",
    "    # print(sentence)\n",
    "    # for pair in pairs:\n",
    "    #     print(pair[0], pair[1])\n",
    "    errors, words = word_error_count(pairs[0][1], sentence)\n",
    "    all_errors.append(errors)\n",
    "    all_words.append(words)\n",
    "    all_pairs.append(pairs)\n",
    "    all_sentences.append(sentence)\n",
    "    all_scores.append(scores.tolist())\n",
    "    c = perf_counter()\n",
    "    time_c += c-b\n",
    "\n",
    "\n",
    "print(time_b, time_c)\n",
    "torch.save({\"errors\": all_errors, \"words\": all_words, \"pairs\": all_pairs, \"sentences\": [s for [s] in all_sentences], \"scores\": all_scores}, \"data.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "bci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
