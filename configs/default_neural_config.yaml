# Mask features
masker:
  mode: timestep      # full to zero out randomly in the featue matrix. timestep to zero out randomly full timesteps. neuron to zero out full neurons
  ratio: 0.5          # ratio of data to predict
  zero_ratio: 0.8     # of the data to predict, ratio of zeroed out
  random_ratio: 0.6   # of the not zeroed, ratio of randomly replaced
  expand_prob: 0.0  # probability of expanding the mask 
  max_timespan: 1     # max span of mask if expanded

# Context available for each timestep
context:
  forward: 10
  backward: 20

# Normalize and add noise
norm_and_noise:
  normalize: false            # normalize the data across the channels before embedding
  norm: "zscore"              # which normalization layer to use (layernorm/scalenorm/zscore)
  eps: 1.e-7                  # avoid dividing by zero when normalizing padded features     
  white_noise_SD: null        # gaussian noise added to the inputs  1.0 originally
  constant_offset_SD: null    # gaussian noise added to the inputs but contsnat in the time dimension 0.2 originally


# Embedding layer
embedder:
  n_channels: 256       # number of neurons recorded 
  n_blocks: 24          # number of blocks of experiments
  n_dates: 24           # number of days of experiments
  max_F: 1024           # max feature len in timesteps

  mode: embed           # linear/embed/identity
  mult: 1               # embedding multiplier. hiddden_sizd = n_channels * mult
  adapt: false          # adapt the embedding layer for each day
  pos: true             # embed position 
  act: identity         # activation for the embedding layers
  bias: true            # use bias in the embedding layer
  dropout: 0.5          # dropout in embedding layer
  
  fixup_init: true      # modify weight initialization
  init_range: 0.1       # initialization range for embeddings
  spike_log_init: false # special initialization 
  max_spikes: 60        # max number of spikes in a single time bin



# Transformer
transformer:
  n_layers: 2           # number of transformer layers
  use_scalenorm: false  # use scalenorm  instead of layernorm
  use_rope: false       # use rotary postional encoding
  rope_theta: 10000.0   # rope angle of rotation
  
  n_heads: 2            # number of attentiomn heads
  attention_bias: false # use bias in the attention layers

  act: relu             # activiation function in mlp layers
  mlp_bias: false       # use bias in the mlp layers
  
  dropout: 0.0          # dropout in transformer layers
  fixup_init: false     # modify weight initialization

# Projection to factor space
factors:  
  project_to_factors: true  # project from hidden_size to factors
  size: 16                # factors size  
  act: relu               # activation function after projecting to factors
  bias: true              # use bias in projection to factors
  dropout: 0.5            # dropout in projection to factors
  fixup_init: true        # modify weight initialization
  init_range: 0.1         # initialization range for factors projetion
  







