seed: 1
savestring: phonemes_ft
data_file: phonemes_data.pth


dirs: include:sc_dirs.yaml

trainer:
  num_epochs: 5
  save_epochs: []
  save_every: 5
  train_len: null     # null to use whole set
  test_len: null      # null to use whole set
  test_batch_size: 64
  train_batch_size: 64

optimizer:
  lr: 1e-4
  wd: 0.01
  warmup_epochs: 0
  scheduler: cosine

lora:
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj","k_proj","v_proj","o_proj"]


# bci: include:configs/default_bci_config.yaml  # this could be null as the bci model already loads this default config but it's good to have it available in the finetuning script
# bci: null


# generation:
#   max_new_tokens: 16
#   do_sample: false
